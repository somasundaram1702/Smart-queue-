{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person_detect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile person_detect.py\n",
    "\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork,IECore\n",
    "from openvino.inference_engine import IEPlugin\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Queue:\n",
    "    '''\n",
    "    Class for dealing with queues\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.queues=[]\n",
    "\n",
    "    def add_queue(self, points):\n",
    "        self.queues.append(points)\n",
    "\n",
    "    def get_queues(self, image):\n",
    "        for q in self.queues:\n",
    "            x_min, y_min, x_max, y_max=q\n",
    "            frame=image[y_min:y_max, x_min:x_max]\n",
    "            yield frame\n",
    "    \n",
    "    def check_coords(self, coords):\n",
    "        d={k+1:0 for k in range(len(self.queues))}\n",
    "        for coord in coords:\n",
    "            for i, q in enumerate(self.queues):\n",
    "                if coord[0]>q[0] and coord[2]<q[2]:\n",
    "                    d[i+1]+=1\n",
    "        return d\n",
    "\n",
    "class PersonDetect:\n",
    "    def __init__(self):\n",
    "        self.plugin = None\n",
    "        self.network = None\n",
    "        self.mod_bin = None\n",
    "        self.mod_xml = None\n",
    "        self.exec_network = None\n",
    "        self.input_blob = None\n",
    "        self.output_blob = None\n",
    "        self.net_input_shape = None\n",
    "        self.frame = None\n",
    "\n",
    "\n",
    "    #Loading the model\n",
    "    def load_model(self,model,device):\n",
    "        self.mod_bin = model+'.bin'\n",
    "        self.mod_xml = model+'.xml'\n",
    "        self.plugin = IECore()\n",
    "        self.network = IENetwork(model=self.mod_xml, weights=self.mod_bin)\n",
    "        self.exec_network = self.plugin.load_network(self.network,device_name=device)\n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        ##Get the supported layers of the network\n",
    "        supp_layers = self.plugin.query_network(network=self.network, device_name = device)\n",
    "\n",
    "        ###Check for any unsupported layers, and let the user\n",
    "        unsupp = [l for l in self.network.layers.keys() if l not in supp_layers]\n",
    "        if len(unsupp) != 0:\n",
    "            print('unsupported layers found: {}'.format(unsupp))\n",
    "            print('Check if the extensions are available in IECore')\n",
    "            exit(1)\n",
    "\n",
    "    def inp_out_blob(self):\n",
    "        self.input_blob = next(iter(self.network.inputs))\n",
    "        self.output_blob = next(iter(self.network.outputs))\n",
    "        self.net_input_shape=self.network.inputs[self.input_blob].shape\n",
    "        return(self.net_input_shape)\n",
    "\n",
    "    def preprocess_input(self, image):\n",
    "        self.x=self.net_input_shape[3]\n",
    "        self.y=self.net_input_shape[2]\n",
    "        self.Y,self.X,_ = np.shape(image)\n",
    "        self.frame_resized = cv2.resize(image, (self.net_input_shape[3], self.net_input_shape[2]))\n",
    "        self.frame = self.frame_resized.transpose((2,0,1))\n",
    "        self.frame = self.frame.reshape(1, *self.frame.shape)\n",
    "        return(self.frame,self.frame_resized)\n",
    "\n",
    "        \n",
    "    def predict(self, image):\n",
    "        self.exec_network.start_async(request_id=0, inputs={self.input_blob: image})\n",
    "        while True:\n",
    "            status = self.exec_network.requests[0].wait(-1)               \n",
    "            if status == 0:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        return(self)\n",
    "\n",
    " \n",
    "    def wait(self):\n",
    "        status = self.exec_network.requests[0].wait(-1)\n",
    "        return status\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.exec_network.requests[0].outputs[self.output_blob]\n",
    "\n",
    "    def preprocess_output(self,result,thresh):\n",
    "        for i in range(len(result[0][0])):\n",
    "            if result[0][0][i][2]>thresh:\n",
    "                #print('entered')\n",
    "                x1,y1,x2,y2 = result[0][0][i][3],result[0][0][i][4],result[0][0][i][5],result[0][0][i][6]\n",
    "                \n",
    "                self.x1 = x1*self.net_input_shape[3]*(self.X/self.x)\n",
    "                self.y1 = y1*self.net_input_shape[2]*(self.Y/self.y)\n",
    "                self.x2 = x2*self.net_input_shape[3]*(self.X/self.x)\n",
    "                self.y2 = y2*self.net_input_shape[2]*(self.Y/self.y)\n",
    "                \n",
    "                yield(int(self.x1),int(self.y1),int(self.x2),int(self.y2))\n",
    "            else:\n",
    "                yield(0,0,0,0)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "#extensions=args.extensions\n",
    "    model= args.model\n",
    "    device=args.device\n",
    "    #visualise=args.visualise\n",
    "    queue_param = args.queue_param\n",
    "    max_people = int(args.max_people)\n",
    "    #visualize = True\n",
    "    video_file=args.video\n",
    "    thresh = float(args.threshold)\n",
    "\n",
    "    start=time.time()\n",
    "    pd=PersonDetect()\n",
    "    pd.load_model(model,device)\n",
    "    mod_inp_shape = pd.inp_out_blob()\n",
    "\n",
    "    print(f\"Time taken to load the model is: {time.time()-start}\")\n",
    "\n",
    "    queue=Queue()\n",
    "    #Queue Parameters\n",
    "    if queue_param == 'retail':\n",
    "       # For retail\n",
    "        queue.add_queue([620, 1, 915, 562])\n",
    "        queue.add_queue([1000, 1, 1264, 461])\n",
    "\n",
    "    if queue_param == 'manufacturing':\n",
    "       # For manufacturing\n",
    "        queue.add_queue([15, 180, 730, 780])\n",
    "        queue.add_queue([921, 144, 1424, 704])\n",
    "\n",
    "    if queue_param == 'transportation':\n",
    "       # For transportation\n",
    "        queue.add_queue([150, 0, 1150, 794])\n",
    "        queue.add_queue([1151, 0, 1915, 841])\n",
    "\n",
    "\n",
    "    cap=cv2.VideoCapture(video_file)\n",
    "    width  = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   \n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    vid_write = cv2.VideoWriter(str(queue_param)+'.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10,(int(width),int(height)))\n",
    "    #print(video_fps)\n",
    "    #print(width,height)\n",
    "    count = 0\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame_org=cap.read()\n",
    "\n",
    "            count+=1\n",
    "            if not ret:\n",
    "                break\n",
    "            w,h,_ = np.shape(frame_org)\n",
    "            #print(w,h)\n",
    "            #for crop_frame in queue.get_queues(frame):\n",
    "\n",
    "            frame, frame_resized = pd.preprocess_input(frame_org)\n",
    "            pd.predict(frame)\n",
    "            cords=[]\n",
    "            # Get the output of inference\n",
    "            if pd.wait() == 0:\n",
    "\n",
    "                result = pd.get_output()\n",
    "                #print(np.shape(result))\n",
    "\n",
    "            for x1,y1,x2,y2 in pd.preprocess_output(result,thresh):\n",
    "                cv2.rectangle(frame_org,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "\n",
    "                cords.append([x1,y1,x2,y2])\n",
    "                #ppl=pd.count_person(result)\n",
    "                #print('Number of people in Queue_'+str(count2)+' is {}'.format(ppl))\n",
    "            #print(np.shape(frame_org))\n",
    "            d = queue.check_coords(cords)\n",
    "            stn_1 = 'No of person in Queue one :'\n",
    "            stn_2 = 'No of person in Queue two :'\n",
    "            cv2.putText(frame_org,stn_1+str(d[1]),(5,100),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3,cv2.LINE_AA)\n",
    "            cv2.putText(frame_org,stn_2+str(d[2]),(5,135),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3,cv2.LINE_AA)\n",
    "\n",
    "            mx_q1 = 'Too many persons in Queue one, move to other queue'\n",
    "            mx_q2 = 'Too many persons in Queue two, move to other queue'    \n",
    "            if d[1] > max_people:\n",
    "                cv2.putText(frame_org,mx_q1,(5,700),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2,cv2.LINE_AA)\n",
    "            if d[2] > max_people:\n",
    "                cv2.putText(frame_org,mx_q2,(5,725),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2,cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            #videowriter = cv2.VideoWriter('Haar.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (500,550))\n",
    "\n",
    "            if args.visualize:\n",
    "                cv2.imshow('output',frame_org)\n",
    "                vid_write.write(frame_org)\n",
    "                cv2.waitKey(1)\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(stn_1+str(d[1]))\n",
    "                print(stn_2+str(d[2]))\n",
    "\n",
    "        print(f\"Time taken for model inference is: {time.time()-start}\")\n",
    "        print('model runs at {} FPS'.format(round(count/(time.time()-start),2)))\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        vid_write.release()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Could not run Inference now\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', required=True)\n",
    "    parser.add_argument('--device', default='CPU')\n",
    "    parser.add_argument('--extensions', default=None)\n",
    "    \n",
    "    parser.add_argument('--visualize', action='store_true')\n",
    "    parser.add_argument('--video', default=None)\n",
    "    parser.add_argument('--queue_param', default=None)\n",
    "    parser.add_argument('--max_people', default=None)\n",
    "    parser.add_argument('--threshold', default=None)\n",
    "    \n",
    "    args=parser.parse_args()\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load the model is: 0.9053595066070557\n",
      "Time taken for model inference is: 14.508329153060913\n",
      "model runs at 11.51 FPS\n"
     ]
    }
   ],
   "source": [
    "!python person_detect.py --model ./intel/person-detection-retail-0013/FP32/person-detection-retail-0013 --device CPU --video ./resources/retail.mp4 --queue_param retail --max_people 1 --threshold 0.8 --visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/somasundaram/Downloads/Udacity/Smart queue Rajalakshmi'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
