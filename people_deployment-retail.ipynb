{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Queue Monitoring System - Retail Scenario\n",
    "\n",
    "In this project, you will build a people counter app to reduce congestion in queuing systems by guiding people to the least congested queue. You will have to use Intel's OpenVINO API and the person detection model from their open model zoo to build this project. It demonstrates how to create a smart video IoT solution using Intel® hardware and software tools. This solution detects people in a designated area, providing the number of people in the frame.\n",
    "\n",
    "## Overview of how it works\n",
    "Your code should read the equivalent of command line arguments and loads a network and image from the video input to the Inference Engine (IE) plugin. A job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute Stick 2 and Intel® Arria® 10 FPGA.\n",
    "After the inference is completed, the output videos are appropriately stored in the /results/[device] directory, which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your dependencies here\n",
    "from demoTools.demoutils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the original video stream used for inference and people counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>People Counter Video</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay muted height=\"480\"><source src=\"./resources/retail.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoHTML('People Counter Video', ['./resources/retail.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Using Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit Inference Engine (IE) to locate people in frame.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Download the model using the open_model_zoo\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the Model using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### 1.1 Downloading Model\n",
    "\n",
    "Write a command to download the  **person-detection-retail-0013** model in an IR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your command here\n",
    "!python /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name person-detection-retail-0013 -o /intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting peron_detect_retail.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile peron_detect_retail.py\n",
    "\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork,IECore\n",
    "from openvino.inference_engine import IEPlugin\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "class Queue:\n",
    "    '''\n",
    "    Class for dealing with queues\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.queues=[]\n",
    "\n",
    "    def add_queue(self, points):\n",
    "        self.queues.append(points)\n",
    "\n",
    "    def get_queues(self, image):\n",
    "        for q in self.queues:\n",
    "            x_min, y_min, x_max, y_max=q\n",
    "            frame=image[y_min:y_max, x_min:x_max]\n",
    "            yield frame\n",
    "    \n",
    "    def check_coords(self, coords):\n",
    "        d={k+1:0 for k in range(len(self.queues))}\n",
    "        for coord in coords:\n",
    "            for i, q in enumerate(self.queues):\n",
    "                if coord[0]>q[0] and coord[2]<q[2]:\n",
    "                    d[i+1]+=1\n",
    "        return d\n",
    "\n",
    "class PersonDetect:\n",
    "    def __init__(self):\n",
    "        self.plugin = None\n",
    "        self.network = None\n",
    "        self.mod_bin = None\n",
    "        self.mod_xml = None\n",
    "        self.exec_network = None\n",
    "        self.input_blob = None\n",
    "        self.output_blob = None\n",
    "        self.net_input_shape = None\n",
    "        self.frame = None\n",
    "\n",
    "\n",
    "#Loading the model\n",
    "    def load_model(self,model,device):\n",
    "        self.mod_bin = model\n",
    "        self.mod_xml = '.'+self.mod_bin.split('.')[1]+'.xml'\n",
    "        self.plugin = IECore()\n",
    "        self.network = IENetwork(model=self.mod_xml, weights=self.mod_bin)\n",
    "        self.exec_network = self.plugin.load_network(self.network,device_name=device)\n",
    "        ##Get the supported layers of the network\n",
    "        supp_layers = self.plugin.query_network(network=self.network, device_name = device)\n",
    "\n",
    "        ###Check for any unsupported layers, and let the user\n",
    "        unsupp = [l for l in self.network.layers.keys() if l not in supp_layers]\n",
    "        if len(unsupp) != 0:\n",
    "            print('unsupported layers found: {}'.format(unsupp))\n",
    "            print('Check if the extensions are available in IECore')\n",
    "            exit(1)\n",
    "        #raise NotImplementedError\n",
    "\n",
    "    def inp_out_blob(self):\n",
    "        self.input_blob = next(iter(self.network.inputs))\n",
    "        self.output_blob = next(iter(self.network.outputs))\n",
    "        self.net_input_shape=self.network.inputs[self.input_blob].shape\n",
    "        return(self.net_input_shape)\n",
    "\n",
    "    def preprocess_input(self, image):\n",
    "        #print(self.net_input_shape)\n",
    "        self.frame = cv2.resize(image, (self.net_input_shape[3], self.net_input_shape[2]))\n",
    "        self.frame = self.frame.transpose((2,0,1))\n",
    "        self.frame = self.frame.reshape(1, *self.frame.shape)\n",
    "        return(self.frame)\n",
    "\n",
    "        \n",
    "    def predict(self, image):\n",
    "        #print('entered predict')\n",
    "        self.exec_network.start_async(request_id=0, inputs={self.input_blob: image})\n",
    "        while True:\n",
    "            status = self.exec_network.requests[0].wait(-1)               \n",
    "            if status == 0:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        return(self)\n",
    "\n",
    " \n",
    "    def wait(self):\n",
    "        status = self.exec_network.requests[0].wait(-1)\n",
    "        return status\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.exec_network.requests[0].outputs[self.output_blob]\n",
    "\n",
    "    def count_person(self,result):\n",
    "        d=0\n",
    "        for i in range(200):\n",
    "            if result[0][0][i][2]>0.5:\n",
    "                d+=1\n",
    "                #print('A person detected')\n",
    "        return(d)\n",
    "\n",
    "\n",
    "'''\n",
    "    def preprocess_outputs(self, outputs):\n",
    "   \n",
    "    raise NotImplementedError\n",
    "'''\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    extensions=args.extensions\n",
    "    model=args.model\n",
    "    device=args.device\n",
    "    #visualise=args.visualise\n",
    "\n",
    "    start=time.time()\n",
    "    pd=PersonDetect()\n",
    "    pd.load_model(model,device)\n",
    "    mod_inp_shape = pd.inp_out_blob()\n",
    "\n",
    "    print(f\"Time taken to load the model is: {time.time()-start}\")\n",
    "    lenr = [[620, 1, 915, 562], [1000, 1, 1264, 461]]\n",
    "    lenm = [[15, 180, 730, 780],[921, 144, 1424, 704]]\n",
    "    lent = [[50, 90, 838, 794],[1000, 74, 1915, 841]]\n",
    "    \n",
    "     #Queue Parameters\n",
    "        # For retail\n",
    "        #queue.add_queue([620, 1, 915, 562])\n",
    "        #queue.add_queue([1000, 1, 1264, 461])\n",
    "        # For manufacturing\n",
    "        #[15, 180, 550, 780]\n",
    "        #[600, 144, 1000, 704]\n",
    "        #queue.add_queue([15, 180, 730, 780])\n",
    "        #queue.add_queue([921, 144, 1424, 704])\n",
    "        # For Transport \n",
    "        #queue.add_queue([50, 90, 838, 794])\n",
    "        #queue.add_queue([852, 74, 1430, 841])\n",
    "\n",
    "\n",
    "    try:\n",
    "        queue=Queue()\n",
    "        queue.add_queue([620, 1, 915, 562])\n",
    "        queue.add_queue([1000, 1, 1264, 461])\n",
    "        video_file=args.video\n",
    "        cap=cv2.VideoCapture(video_file)\n",
    "        count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame=cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            count+=1\n",
    "            count2 = 0\n",
    "            for crop_frame in queue.get_queues(frame):\n",
    "                cv2.imwrite('./out_check/manu_'+str(count2)+'.jpg',crop_frame)\n",
    "                frame = pd.preprocess_input(crop_frame)\n",
    "                pd.predict(frame)\n",
    "                # Get the output of inference\n",
    "                if pd.wait() == 0:\n",
    "                    count2+=1\n",
    "                    result = pd.get_output()\n",
    "                    ppl=pd.count_person(result)\n",
    "                    print('Number of people in Queue_'+str(count2)+' is {}'.format(ppl))                 \n",
    "        \n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    except Exception as e:\n",
    "        print(\"Could not run Inference now\", e)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', required=True)\n",
    "    parser.add_argument('--device', default='CPU')\n",
    "    parser.add_argument('--extensions', default=None)\n",
    "    \n",
    "    parser.add_argument('--visualise', default=None)\n",
    "    parser.add_argument('--video', default=None)\n",
    "    parser.add_argument('--queue_param', default=None)\n",
    "    parser.add_argument('--max_people', default=None)\n",
    "    parser.add_argument('--threshold', default=None)\n",
    "    \n",
    "    args=parser.parse_args()\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Inference on a video\n",
    "\n",
    "By now you should have already completed the inference code in <a href=\"person_detect.py\">person_detect.py</a>. If you haven't done so already, then you should do it now.\n",
    "\n",
    "The Python code should take in command line arguments for video, model etc.\n",
    "\n",
    "While the type of command line options is up to you, the command below is an example \n",
    "\n",
    "```\n",
    "python3 main.py -m ${MODELPATH} \\\n",
    "                -i ${INPUT_FILE} \\\n",
    "                -o ${OUTPUT_FILE} \\\n",
    "                -d ${DEVICE} \\\n",
    "                -pt ${THRESHOLD}\\\n",
    "\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the pre-trained IR model which has been pre-processed using the model optimizer. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "* -i  location of the input video stream\n",
    "* -o location where the output file with inference needs to be stored (results/[device])\n",
    "* -d type of Hardware Acceleration (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "* -pt probability threshold value for the person detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating job file\n",
    "\n",
    "To run inference on the video, we need more compute power.\n",
    "We will run the workload on several edge compute nodes present in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "You will have to create the job file by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile person_detect_retail_job.sh\n",
    "# The writefile magic command can be used to create and save a file\n",
    "\n",
    "MODEL=$1\n",
    "DEVICE=$2\n",
    "VIDEO=$3\n",
    "QUEUE=$4\n",
    "OUTPUT=$5\n",
    "PEOPLE=$6\n",
    "\n",
    "mkdir -p $5\n",
    "\n",
    "if [ $DEVICE = \"HETERO:FPGA,CPU\" ]; then\n",
    "    #Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/intel/init_openvino.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP16_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "python3 person_detect_retail.py  --model ${MODEL} \\\n",
    "                                --visualise \\\n",
    "                                --queue_param ${QUEUE} \\\n",
    "                                --device ${DEVICE} \\\n",
    "                                --video ${VIDEO}\\\n",
    "                                --output_path ${OUTPUT}\\\n",
    "                                --max_people ${PEOPLE} \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit people_counter to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option let us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option let us send arguments to the bash script. \n",
    "- `-N` : this option let us name the job so that it is easier to distinguish between them.\n",
    "\n",
    "Example using `qsub` command:\n",
    "\n",
    "`!qsub person_detect_job.sh -l nodes=1:tank-870:i5-6500te -d . -F \"models/intel/PATH-TO-MODEL DEVICE resources/retail.mp4 bin/queue_param/retail.npy results/retail/DEVICE MAX-PEOPLE\" -N JOB-NAME`\n",
    "\n",
    "You will need to change the following variables, `models/intel/PATH-TO-MODEL`, `DEVICE`, `results/retail/DEVICE`, `MAX-PEOPLE`, and `JOB-NAME` to the appropriate values.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Job queue submission\n",
    "\n",
    "Each of the cells below should submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all jobs at once or one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "If your job successfully runs and completes, it will output a video, `output_video.mp4`, and a text file, `stats.txt`, in the `results/retail/DEVICE` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core™ i5-6500TE processor</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = !/home/workspace/qsub person_detect_retail_job.sh -d . -l nodes=1:tank-870:i5-6500te -F \"./intel/person-detection-retail-0013/FP32/person-detection-retail-0013.bin ./resources/retail.mp4\" -N store_core \n",
    "print(job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP32 used in CPU and F16 is used in VPU, FPGA,GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = !qsub person_detect_manu_job.sh -d . -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"GPU ./intel/person-detection-retail-0013/FP32/person-detection-retail-0013.bin ./resources/retail.mp4\" -N store_core \n",
    "print(job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = !/home/workspace/qsub person_detect_manu_job.sh -d . -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"MYRIAD ./intel/person-detection-retail-0013/FP16/person-detection-retail-0013.bin ./resources/retail.mp4\" -N store_core \n",
    "print(job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core™ i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id = !/home/workspace/qsub person_detect_manu_job.sh -d . -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"HETERO:FPGA,CPU ./intel/person-detection-retail-0013/FP16/person-detection-retail-0013.bin ./resources/retail.mp4\" -N store_core \n",
    "print(job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use a command to check the status of the job.\n",
    "\n",
    "Column `S` shows the state of your running jobs.\n",
    "\n",
    "For example:\n",
    "- If `JOB ID`is in Q state, it is in the queue waiting for available resources.\n",
    "- If `JOB ID` is in R state, it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liveQStat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6f2715dd1d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Enter your command here to check the status of your jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#import liveQStat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mliveQStat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliveQstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'liveQStat' is not defined"
     ]
    }
   ],
   "source": [
    "# Enter your command here to check the status of your jobs\n",
    "#import liveQStat\n",
    "liveQStat.liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering to complete before proceeding to the next step.\n",
    "\n",
    "## Step 3: View Results\n",
    "\n",
    "Write a short utility script that will display these videos within the notebook.\n",
    "\n",
    "*Tip*: See `demoutils.py` if you are interested in understanding further on how the results are displayed in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write your script for Intel Core CPU video results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your script for Intel Core CPU +GPU video results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your script for Intel CPU + Intel NCS2 video results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your script for Intel® Arria® 10 FPGA video results here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Assess Performance\n",
    "\n",
    "This is where you need to write code to asses how well your model is performing. You will use the `stats.txt` file located in your results directory.\n",
    "You need to compare the following timings for all the models across all 4 devices:\n",
    "\n",
    "- Model loading time\n",
    "- Average Inference Time\n",
    "- FPS\n",
    "\n",
    "Show your results in the form of a bar chart using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Write your code here for model loading time on all 4 device types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Write your code here for model average inference time on all 4 device types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Write your code here for model FPS on all 4 device types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
